---
title: "Week 8 - 9"
author: "Kimberly Cable"
date: "05-14-2022"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 6

```{r echo = FALSE}
## Load the `data/r4ds/heights.csv` to
heights_df <- read.csv("data/r4ds/heights.csv")
head(heights_df)
```

```{r echo = FALSE}
## Load the ggplot2 library
library(ggplot2)
```

## Fit a linear model using the `age` variable as the predictor and `earn` as the outcome

lm(outcome ~ predictors, data = dataframe)
```{r}
age_lm <-  lm(earn ~ age, data = heights_df)
```

## View the summary of your model using `summary()`
```{r}
summary(age_lm)
```

## Creating predictions using `predict()`
```{r}
age_predict_df <- data.frame(earn = predict(age_lm, heights_df), age = heights_df$age)
head(age_predict_df)
```

## Plot the predictions against the original data
```{r}
ggplot(data = heights_df, aes(y = earn, x = age)) +
  geom_point(color = 'blue') +
  geom_line(color='red',data = age_predict_df, aes(y = earn, x = age))
```

```{r}
mean_earn <- mean(heights_df$earn)
mean_earn
```

## Corrected Sum of Squares Total
```{r}
sst <- sum((mean_earn - heights_df$earn)^2)
sst
```

## Corrected Sum of Squares for Model
```{r}
ssm <- sum((mean_earn - age_predict_df$earn)^2)
ssm
```

## Residuals
```{r}
residuals <- heights_df$earn - age_predict_df$earn
head(residuals)
```

## Sum of Squares for Error
```{r}
sse <- sum(residuals^2)
sse
```

## R Squared R^2 = SSM/SST
```{r}
r_squared <- ssm / sst
r_squared
```

## Number of observations
```{r}
n <- nrow(heights_df)
n
```

## Number of regression parameters
```{r}
p <- 2
```

## Corrected Degrees of Freedom for Model (p-1)
```{r}
dfm <- p - 1
```

## Degrees of Freedom for Error (n-p)
```{r}
dfe <- n - p
```

## Corrected Degrees of Freedom Total:   DFT = n - 1
```{r}
dft <- n - 1
```

## Mean of Squares for Model:   MSM = SSM / DFM
```{r}
msm <- ssm / dfm
msm
```

## Mean of Squares for Error:   MSE = SSE / DFE
```{r}
mse <- sse / dfe
mse
```

## Mean of Squares Total:   MST = SST / DFT
```{r}
mst <- sst / dft
mst
```

## F Statistic F = MSM/MSE
```{r}
f_score <- msm / mse
f_score
```

## Adjusted R Squared R2 = 1 - (1 - R2)(n - 1) / (n - p)
```{r}
adjusted_r_squared <- 1 - ((1 - r_squared) * dft) / dfe
adjusted_r_squared
```

## Calculate the p-value from the F distribution
```{r}
p_value <- pf(f_score, dfm, dft, lower.tail = F)
p_value
```
# Assignment 7

## Load the `data/r4ds/heights.csv` to
```{r echo=FALSE}
heights_df <- read.csv("data/r4ds/heights.csv")
head(heights_df)
```

# Fit a linear model
```{r}
earn_lm <-  lm(earn ~ height + sex + ed + age + race, data = heights_df)
earn_lm
```

# View the summary of your model
```{r}
summary(earn_lm)
```

```{r}
predicted_df <- data.frame(
  earn = predict(earn_lm, heights_df),
  ed = heights_df$age, 
  race = heights_df$race, 
  height = heights_df$height,
  age = heights_df$age, 
  sex = heights_df$sex
  )
head(predicted_df)
```

## Compute deviation (i.e. residuals)
```{r}
mean_earn <- mean(heights_df$earn)
mean_earn
```

## Corrected Sum of Squares Total
```{r}
sst <- sum((mean_earn - heights_df$earn)^2)
sst
```

## Corrected Sum of Squares for Model
```{r}
ssm <- sum((mean_earn - predicted_df$earn)^2)
ssm
```

## Residuals
```{r}
residuals <- heights_df$earn - predicted_df$earn
head(residuals)
```

## Sum of Squares for Error
```{r}
sse <- sum(residuals^2)
sse
```

## R Squared
```{r}
r_squared <- ssm / sst
r_squared
```

## Number of observations
```{r}
n <- nrow(heights_df)
n
```

## Number of regression parameters
```{r}
p <- 8
```

## Corrected Degrees of Freedom for Model
```{r}
dfm <- p - 1
dfm
```

## Degrees of Freedom for Error
```{r}
dfe <- n - p
dfe
```

## Corrected Degrees of Freedom Total:   DFT = n - 1
```{r}
dft <- n - 1
dft
```

## Mean of Squares for Model:   MSM = SSM / DFM
```{r}
msm <- ssm / dfm
msm
```

## Mean of Squares for Error:   MSE = SSE / DFE
```{r}
mse <- sse / dfe
mse
```

## Mean of Squares Total:   MST = SST / DFT
```{r}
mst <- sst / dft
mst
```

## F Statistic
```{r}
f_score <- msm / mse
f_score
```

## Adjusted R Squared R2 = 1 - (1 - R2)(n - 1) / (n - p)
```{r}
adjusted_r_squared <- 1 - ((1 - r_squared) * dft) / dfe
adjusted_r_squared
```
# Housing Data

```{r, include = FALSE}
library(readxl)
library(dplyr)
library(magrittr)
library(Hmisc)
library(lm.beta)
library(car)

options(scipen=999)
```


### Data for this assignment is focused on real estate transactions recorded from 1964 to 2016 and can be found in Housing.xlsx. Using your skills in statistical correlation, multiple regression, and R programming, you are interested in the following variables: Sale Price and several other possible predictors.

### If you worked with the Housing dataset in previous week â€“ you are in luck, you likely have already found any issues in the dataset and made the necessary transformations. If not, you will want to take some time looking at the data with all your new skills and identifying if you have any clean up that needs to happen.

## Load the `data/week-6-housing.csv` to
```{r, echo = FALSE}
housing_df = read_excel("data/week-6-housing.xlsx")
housing_df %>% head(4)
```


### i Explain any transformations or modifications you made to the dataset

I created a variable names total_bath_count that combined bath_full, bath_half, and bath_3qr

I renamed Sale Price to Sale_Price

I renamed Sale Date to Sale_Date

```{r, echo = FALSE}
num_baths <- function(bath_full, bath_half, bath_3qtr)
{
    ifelse (bath_full > 0, total <- bath_full * 1, NA)
    
    ifelse (bath_half > 0, total <- total + (bath_half * 0.5), NA)
    
    ifelse (bath_3qtr > 0, total <- total + (bath_3qtr * 0.75), NA)
    
    return (total)
}

housing_df['total_bath_count'] <- num_baths(
    bath_full = housing_df$bath_full_count, 
    bath_half = housing_df$bath_half_count, 
    bath_3qtr = housing_df$bath_3qtr_count)

colnames(housing_df)[2] <- "Sale_Price"
colnames(housing_df)[1] <- "Sale_Date"

housing_df %>% head(4)
```


### ii: Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.

For my additional predictors, I chose total_bath_count, square_feet_total_living, and bedrooms because they traditionally are used to calculate the price of a house

```{r}
housing_df_model1 <- lm(Sale_Price ~ sq_ft_lot, data = housing_df)


housing_df_model2 <- lm(Sale_Price ~ total_bath_count + square_feet_total_living + 
                            bedrooms, data = housing_df)
```


### iii: Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r, echo = FALSE}
summary(housing_df_model1)
```

The linear regression model for Sales Price and Square Feet per Lot is an R2 = 0.01435 tells us sq feet per lot only accounts for 1.4% of the variation in the Sales Price.

```{r, echo = FALSE}
summary(housing_df_model2)
```

The linear regression model for Sales Price, total_bath_count, square_feet_total_living, and bedrooms is an R2 = 0.2096 tells us that adding other variables accounts for 1.2% of the price of a house.

The adjusted R2 with 1 variable is 0.01428 and with 3 variables it is 0.295 which indicates that adding the additional variables does help but not a lot.


### iii: Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r, echo = FALSE}
summary(housing_df_model1)
```

The linear regression model for Sales Price and Square Feet per Lot is an R2 = 0.01435 tells us sq feet per lot only accounts for 1.4% of the variation in the Sales Price.

```{r, echo = FALSE}
summary(housing_df_model2)
```

The linear regression model for Sales Price, total_bath_count, square_feet_total_living, and bedrooms is an R2 = 0.2096 tells us that adding other variables accounts for 1.2% of the price of a house.

The adjusted R2 with 1 variable is 0.01428 and with 3 variables it is 0.295 which indicates that adding the additional variables does help but not a lot.

### iv: Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?

```{r}
beta_model1 <- lm.beta(housing_df_model1)
beta_model1
```


Standard Deviation of Sale_Price:

```{r, echo = FALSE}
sd(housing_df$Sale_Price)
```

Standard Deviation of sq_ft_lot:

```{r, echo = FALSE} 
sd(housing_df$sq_ft_lot)
```

```{r}
beta_model2 <- lm.beta(housing_df_model2)
beta_model2
```

Standard Deviation of total_bath_count:

```{r, echo = FALSE} 
sd(housing_df$total_bath_count)
```

As the number of bath increase by 1 standard deviation (0.70), Sales Price increase by 0.49 standard deviations.  So for every 0.7 bathrooms, Sales Price goes up by ($404,381 * 0.049) $19,815.

Standard Deviation of square_feet_total_living:

```{r, echo = FALSE}
sd(housing_df$square_feet_total_living)
```

As the number of sq ft of living space increase by 1 standard deviation (990 sq ft), Sales Price increases by 0.453 standard deviations. So for every 990 sq ft added, Sales Price goes up by ($404,381 * 0.453) $183,185.

Standard Deviation of bedrooms:

```{r, echo = FALSE}
sd(housing_df$bedrooms)
```

As the number of bedrooms increases by 1 standard deviation (.88), Sales Price decrease by -0.060 standard deviations. So for every .88 bedrooms added, Sales Price goes down by ($404,381 * -0.06) $24,263.


### v: Calculate the confidence intervals for the parameters in your model and explain what the results indicate.

```{r}
model2.confInt <- confint(housing_df_model2)
model2.confInt
```

None of the confidence intervals cross 0 but their intervals are not very small so they are less representative in calculating the sales price but are significant

### vi: Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.

```{r, echo = FALSE}
anova(housing_df_model1, housing_df_model2)
```

### vii: Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

```{r}
housing_df$residuals <- resid(housing_df_model2)
housing_df$standarized.residuals <- rstandard(housing_df_model2)
housing_df$studentized.residuals <- rstudent(housing_df_model2)
housing_df$cooks.distance <- cooks.distance(housing_df_model2)
housing_df$dfbeta <- dfbeta(housing_df_model2)
housing_df$dffit <- dffits(housing_df_model2)
housing_df$leverage <- hatvalues(housing_df_model2)
housing_df$covariance.ratios <- covratio(housing_df_model2)

head(housing_df)
```

### viii: Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.

```{r}
housing_df$large.residuals <- housing_df$standarized.residuals > 2 | 
  housing_df$standarized.residuals < -2
```

### ix: Use the appropriate function to show the sum of large residuals.

```{r}
sum(housing_df$large.residuals)
```
There are 322 cases that have large residuals. So only 2.5% (322/ 12865) of the data was outside the limits.

### x: Which specific variables have large residuals (only cases that evaluate as TRUE)?

```{r}
housing_df[housing_df$large.residuals, c('Sale_Price', 'total_bath_count', 
                                         'square_feet_total_living', 'bedrooms', 
                                         'standarized.residuals')]
```

### xi: Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.

```{r}
large_residuals <- housing_df[housing_df$large.residuals, 
                              c('cooks.distance', 'leverage', 
                                'covariance.ratios')]
large_residuals
```

No Cooks distance is > 1 so none of the cases is having an undue influence on the model.

```{r}
leverage_threshold <- (4 / 12865) * 4

large_residuals['large.leverage'] <- 
  large_residuals$leverage > leverage_threshold

sum(large_residuals$large.leverage)
```
The average threshold is .0012 so there are only 45 cases greater than the average. With this we seems to have a fairly reliable model that hasnt been unduly influenced by any subset of cases.


### xii: Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.

```{r}
durbinWatsonTest(housing_df_model2)
```

The durbin watson statistic is 0.539 which is not close to 2 so it has not been met


### xiii: Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.

```{r}
vif(housing_df_model2)

1 / vif(housing_df_model2)

mean(vif(housing_df_model2))
```

The VIF for each variable is below 10, and average VIF for each variable is greater than 1, and the tolerance for all three are above 0.2, and the average VIF is close to 1 so we can conclude that there is no collinearity within the data.


### xiv: Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.

```{r, echo = FALSE}
hist(housing_df$studentized.residuals)
```

```{r, echo = FALSE}
plot(housing_df_model2)
```


The plots show that is it not normal and skewed

### xv: Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

The model overall is unbiased and should be ok for the general population.
